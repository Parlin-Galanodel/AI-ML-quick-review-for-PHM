<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 19: Reinforcement Learning | AI for the Physicist</title>
    
    <link rel="stylesheet" href="css/style.css">
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true},{left: '$', right: '$', display: false}]});"></script>

    <!-- Mermaid.js for rendering diagrams -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
</head>
<body>

    <div class="container">
        <header>
            <p><a href="index.html">← Back to Table of Contents</a></p>
            <h2>Chapter 19: Reinforcement Learning – Learning to Act Optimally</h2>
        </header>

        <main>
            <p>Reinforcement Learning (RL) provides a mathematical framework for learning goal-oriented behavior through trial-and-error interaction. It is the language of <strong>optimal control theory</strong>, a field with deep roots in physics and engineering. RL provides a path towards building agents that can make optimal decisions in complex, dynamic environments, such as controlling a particle accelerator, discovering a new chemical catalyst, or navigating a robot through a complex space.</p>

            <h4>19.1 The Formalism of Markov Decision Processes</h4>
            <p>The standard mathematical formulation for an RL problem is the <strong>Markov Decision Process (MDP)</strong>. An MDP provides the formal language to describe the interaction between a learning agent and its environment. It is defined by a tuple $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$:</p>
            <ul>
                <li>$\mathcal{S}$: A set of possible <strong>states</strong> of the environment. For a particle in a box, this could be its position and momentum $(\mathbf{q}, \mathbf{p})$.</li>
                <li>$\mathcal{A}$: A set of possible <strong>actions</strong> the agent can take. This is the set of controls one can apply, like applying a force or adjusting a magnetic field.</li>
                <li>$P(s'|s,a)$: The <strong>transition probability function</strong>. This defines the dynamics of the system, giving the probability of transitioning to state $s'$ after taking action $a$ in state $s$.</li>
                <li>$R(s,a,s')$: The <strong>reward function</strong>. It gives the immediate scalar reward received for a transition. This is the objective signal the agent seeks to maximize.</li>
                <li>$\gamma$: The <strong>discount factor</strong>, which trades off the importance of immediate versus future rewards.</li>
            </ul>
            <p>The agent's goal is to learn a <strong>policy</strong>, $\pi(a|s)$, which is a strategy for choosing actions. An optimal policy, $\pi^*$, is one that maximizes the expected <strong>return</strong> (the discounted sum of future rewards).</p>

            <h4>19.2 The Bellman Equations: A Self-Consistency Condition</h4>
            <p>To find the optimal policy, we first define value functions. The <strong>state-value function</strong>, $V^\pi(s)$, is the expected return when starting in state $s$ and following policy $\pi$. The value functions obey a crucial recursive relationship known as the <strong>Bellman equations</strong>. These equations express the value of a state in terms of the values of its successor states, forming a self-consistency condition that is the foundation for almost all RL algorithms. For the optimal value function $V^*(s)$, the Bellman optimality equation is:</p>
            $$ V^*(s) = \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma V^*(s') \right] $$
            <p>This equation states that the value of the best state is equal to the reward you get from taking the best possible action, plus the discounted value of the best state you can land in. If we know the full dynamics ($P$ and $R$), we can solve this system to find the optimal policy.</p>

            <h4>19.3 Model-Free Learning: Learning from Experience</h4>
            <p>In most realistic scenarios, the dynamics of the environment are a black box. <strong>Model-free</strong> RL algorithms learn the optimal policy directly from experience—that is, from samples of transitions $(s, a, r, s')$ generated by interacting with the environment.</p>
            
            <blockquote>
                <p><strong>Analogy: Navigating a Maze</strong></p>
                <p>Imagine an agent in a maze where some exits give rewards. The agent doesn't have a map (the model, $P$). It has to learn how to navigate by trial and error.</p>
                <ul>
                    <li><strong>Monte Carlo (MC) Methods:</strong> The agent wanders randomly through the maze until it finds an exit. It then goes back and strengthens every decision that led to a good exit, and weakens every decision that led to a bad one. It learns only at the end of a full "episode." This is unbiased (it uses the true, full reward) but high-variance (a single lucky path can skew its beliefs).</li>
                    <li><strong>Temporal-Difference (TD) Learning:</strong> The agent takes a single step. It doesn't know the final outcome, but it might notice that its new location feels "better" (maybe it's a square it has previously learned is often on a good path). It updates its belief about the previous square based on this one-step lookahead. It learns from every single step, a process called "bootstrapping." This is lower-variance but biased (since its belief about the next square might be wrong).</li>
                </ul>
            </blockquote>
            
            <p><strong>Q-learning</strong> is a breakthrough off-policy TD control algorithm. It learns the optimal <strong>action-value function</strong>, $Q^*(s,a)$, which is the expected return for taking action $a$ in state $s$ and then behaving optimally thereafter. Its update rule is:</p>
            $$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t) \right] $$
            <p>The "off-policy" aspect is crucial: it allows the agent to explore the environment (e.g., by taking some random actions) while still learning about the optimal, greedy policy (because the update rule always uses the $\max_{a'}$ over the next state's actions).</p>

            <h4>19.4 Deep Reinforcement Learning</h4>
            <p>For problems with very large or continuous state spaces (e.g., learning from raw pixels), we cannot use a simple table to store all the Q-values. <strong>Deep Reinforcement Learning</strong> uses a deep neural network as a function approximator to represent the value function or the policy.</p>
            
            <h5>Deep Q-Networks (DQN)</h5>
            <p>The DQN algorithm replaces the Q-table with a deep neural network (often a CNN) that takes the state $s$ as input and outputs a Q-value for each possible action. Training a Q-network with TD learning is notoriously unstable. The DQN paper introduced two key innovations to stabilize it:</p>
            <ol>
                <li><strong>Experience Replay:</strong> The agent stores its experiences $(s, a, r, s')$ in a large replay buffer. During training, instead of using the most recent transition, the algorithm samples random mini-batches from this buffer. This breaks the strong temporal correlations in the sequence of experiences, making the training samples more independent and identically distributed (i.i.d.).</li>
                <li><strong>Target Network:</strong> The Q-values in the TD target ($R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a')$) are computed using a separate <strong>target network</strong>. This target network is a frozen copy of the main network from an earlier time step. It is only updated periodically. This prevents the target from chasing a moving value, which can lead to oscillations and divergent behavior. It provides a stable, stationary target for the main Q-network to converge towards.</li>
            </ol>

            <h5>Policy Gradient Methods and Actor-Critic</h5>
            <p>An alternative to value-based methods is to learn the policy directly. <strong>Policy Gradient</strong> methods parameterize the policy itself with a neural network, $\pi_{\boldsymbol{\theta}}(a|s)$, that outputs a probability distribution over actions. The goal is to perform gradient ascent on the expected total return. The <strong>Policy Gradient Theorem</strong> provides the crucial update rule:</p>
            $$ \nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) \propto \mathbb{E}_{\tau \sim \pi_{\boldsymbol{\theta}}} [ A(S_t, A_t) \nabla_{\boldsymbol{\theta}} \log \pi_{\boldsymbol{\theta}}(A_t|S_t)] $$
            <p>In simple terms: increase the log-probability of taking an action in proportion to how much better that action was than the average action in that state. The term $A(S_t, A_t) = Q(s,a) - V(s)$ is the <strong>advantage function</strong>. This leads to <strong>Actor-Critic</strong> algorithms:</p>
            <ul>
                <li>An <strong>Actor</strong> network learns the policy, $\pi_{\boldsymbol{\theta}}(a|s)$.</li>
                <li>A <strong>Critic</strong> network learns a value function, $V_{\boldsymbol{\phi}}(s)$, which is used to estimate the advantage, providing a stable, low-variance signal for updating the actor.</li>
            </ul>
            <p>This hybrid approach is the foundation for many of the most powerful modern RL agents.</p>
        </main>
        
        <hr>
        
        <nav class="navigation-links" style="display: flex; justify-content: space-between;">
            <a href="ch18.html">← Chapter 18</a>
            <a href="ch20.html">Chapter 20: Physics-Informed ML →</a>
        </nav>
    </div>

</body>
</html>